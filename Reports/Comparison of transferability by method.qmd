---
title: "Comparison of model transferability by method"
format: 
 pdf:
  toc: true
  number-sections: true
  number-depth: 2
  colorlinks: true
  fig-pos: 'H'
  fig-align: center
  fig-dpi: 300
  fig-format: png
execute: 
  echo: false
  warning: false
  message: false
editor_options: 
  chunk_output_type: console
---

## Background

The first objective of my NSF project is to compare among four different modeling methods trained on Gulf of Mexico green turtle data and determine which produces the most accurate predictions of habitat selection on independent data sets from Brazil and Qatar. Each model was fit using a resource selection function, typically expressed as $w(x) = exp(\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots)$. The modeling methods that were compared include a generalized linear model (GLM), generalized additive model (GAM), boosted regression tree (BRT), and Gaussian Process regression (GPR). To account for individual functional responses to each of the selected model covariates (i.e., bathymetric depth, net primary productivity, sea surface temperature), each model was fit as a hierarchical model using a random intercept and random slopes per each of the 49 individuals tracked in the GoM, with the exception of the BRT model since there is not currently a way to implement such effects. These models are therefore abbreviated as HGLM, HGAM, and HGPR. To allow the HGLM model to more flexibly fit non-linear relationships, quadratic terms were included for each covariate.

```{r}
#| label: load-data


### Compare model transferability by method ###

library(tidyverse)
library(INLA)
library(mgcv)
library(gbm)
library(terra)
library(sfarrow)
library(tictoc)
library(lubridate)
library(cmocean)
library(patchwork)
library(tidyterra)

source('../Scripts/helper functions.R')


#######################
### Load GoM tracks ###
#######################

rsf.pts_10 <- read_csv("../Processed_data/GoM_Cm_RSFprep_10x.csv")


##########################
### Load fitted models ###
##########################

hglm.fit <- readRDS("../Data_products/HGLM_model_fit.rds")
hgam.fit <- readRDS("../Data_products/HGAM_model_fit.rds")
brt.fit <- readRDS("../Data_products/BRT_model_fit.rds")
hgpr.fit <- readRDS("../Data_products/HGPR_model_fit.rds")




################################
### Load validation datasets ###
################################

dat.br <- read_csv("../Processed_data/Brazil_Cm_Tracks_behav.csv")
dat.qa <- read_csv("../Processed_data/Qatar_Cm_Tracks_behav.csv")

# Create indexing column "month.year" and only retain Resident locs
dat.br <- dat.br %>%
  mutate(month.year = as_date(date), .after = 'date') %>%
  mutate(month.year = str_replace(month.year, pattern = "..$", replacement = "01")) %>%
  filter(behav == 'Resident')

dat.qa <- dat.qa %>%
  mutate(month.year = as_date(date), .after = 'date') %>%
  mutate(month.year = str_replace(month.year, pattern = "..$", replacement = "01")) %>%
  filter(behav == 'Resident')


# Load spatial land layers
gom.sf <- st_read_parquet("../Environ_data/GoM_land.parquet")
br.sf <- st_read_parquet("../Environ_data/Brazil_land.parquet")
qa.sf <- st_read_parquet("../Environ_data/Qatar_land.parquet")




########################################
### Load environmental raster layers ###
########################################

### Gulf of Mexico ###

## Load in environ rasters
files <- list.files(path = '../Environ_data', pattern = "GoM", full.names = TRUE)
files <- files[!grepl(pattern = "example", files)]  #remove any example datasets
files <- files[!grepl(pattern = "Kd490", files)]  #remove Kd490 datasets
files <- files[grepl(pattern = "tif", files)]  #only keep GeoTIFFs

# Merge into list; each element is a different covariate
cov_list <- sapply(files, rast)
names(cov_list) <- c('bathym', 'npp', 'sst')

# Change names for dynamic layers to match YYYY-MM-01 format
for (var in c('npp', 'sst')) {
  names(cov_list[[var]]) <- gsub(names(cov_list[[var]]), pattern = "-..$", replacement = "-01")
}


## Set all positive bathymetric values (i.e., elevation) as NA
cov_list[["bathym"]][cov_list[["bathym"]] > 0] <- NA


## Transform raster layers to match coarsest spatial resolution (i.e., NPP)
for (var in c("bathym", "sst")) {
  cov_list[[var]] <- terra::resample(cov_list[[var]], cov_list$npp, method = "average")
}

## Deal w/ bathym depth exactly equal to 0 (since a problem on log scale)
cov_list[["bathym"]][cov_list[["bathym"]] > -1e-9] <- NA


## Transform CRS to match tracks
cov_list <- map(cov_list, terra::project, 'EPSG:3395')




### Brazil ###

## Load in environ rasters
files <- list.files(path = '../Environ_data', pattern = "Brazil", full.names = TRUE)
files <- files[grepl(pattern = "tif", files)]  #only keep GeoTIFFs

# Merge into list; each element is a different covariate
cov_list_br <- sapply(files, rast)
names(cov_list_br) <- c('bathym', 'npp', 'sst')

# Change names for dynamic layers to match YYYY-MM-01 format
for (var in c('npp', 'sst')) {
  names(cov_list_br[[var]]) <- gsub(names(cov_list_br[[var]]), pattern = "-..$", replacement = "-01")
}

# Set all positive bathymetric values (i.e., elevation) as NA
cov_list_br[["bathym"]][cov_list_br[["bathym"]] > 0] <- NA

# Transform raster layers to match coarsest spatial resolution (i.e., NPP)
for (var in c("bathym", "sst")) {
  cov_list_br[[var]] <- terra::resample(cov_list_br[[var]], cov_list_br$npp, method = "average")
}

# Deal w/ bathym depth exactly equal to 0 (since a problem on log scale)
cov_list_br[["bathym"]][cov_list_br[["bathym"]] > -1e-9] <- NA


# Transform CRS to match tracks
cov_list_br <- map(cov_list_br, terra::project, 'EPSG:3395')






### Qatar ###

## Load in environ rasters
files <- list.files(path = '../Environ_data', pattern = "Qatar", full.names = TRUE)
files <- files[grepl(pattern = "tif", files)]  #only keep GeoTIFFs

# Merge into list; each element is a different covariate
cov_list_qa <- sapply(files, rast)
names(cov_list_qa) <- c('bathym', 'npp', 'sst')

# Change names for dynamic layers to match YYYY-MM-01 format
for (var in c('npp', 'sst')) {
  names(cov_list_qa[[var]]) <- gsub(names(cov_list_qa[[var]]), pattern = "-..$", replacement = "-01")
}

# Set all positive bathymetric values (i.e., elevation) as NA
cov_list_qa[["bathym"]][cov_list_qa[["bathym"]] > 0] <- NA

# Transform raster layers to match coarsest spatial resolution (i.e., NPP)
for (var in c("bathym", "sst")) {
  cov_list_qa[[var]] <- terra::resample(cov_list_qa[[var]], cov_list_qa$npp, method = "average")
}

# Deal w/ bathym depth exactly equal to 0 (since a problem on log scale)
cov_list_qa[["bathym"]][cov_list_qa[["bathym"]] > -1e-9] <- NA


# Transform CRS to match tracks
cov_list_qa <- map(cov_list_qa, terra::project, 'EPSG:3395')
```

Below, I've shown rasters for each of the three covariates (i.e., depth, NPP, SST) in October 2011 for the Gulf of Mexico. Additionally, I've shown predicted surfaces of log(intensity) of use per model for the GoM.

```{r}
#| label: map-gom-covs
#| column: page
#| fig-cap: "Example raster layers from the Gulf of Mexico for each of the selected covariates included in the model. Since NPP and SST are dynamic variables, these data were accessed on a monthly basis."

# Define map extent
bbox <- ext(cov_list$bathym)

p.gom.bathym <- ggplot() +
  geom_spatraster(data = cov_list$bathym, aes(fill = `GoM bathymetry`)) +
  scale_fill_cmocean("Depth (m)", name = "deep", direction = -1) +
  geom_sf(data = gom.sf) +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme_bw()

p.gom.npp <- ggplot() +
  geom_spatraster(data = cov_list$npp, aes(fill = `2011-10-01`)) +
  scale_fill_cmocean(expression(paste("NPP (", mg~C~m^-2~d^-1, ")")), name = "algae", direction = 1) +
  geom_sf(data = gom.sf) +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme_bw()

p.gom.sst <- ggplot() +
  geom_spatraster(data = cov_list$sst, aes(fill = `2011-10-01`)) +
  scale_fill_cmocean("SST (Â°C)", name = "thermal", direction = 1) +
  geom_sf(data = gom.sf) +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme_bw()

# Make composite plot
p.gom.bathym + p.gom.npp + p.gom.sst + 
  plot_layout(ncol = 1) +
  plot_annotation(title = 'Environmental values for 2011-10')
```

```{r}
#| label: gom-preds

## HGLM

newdat <- data.frame(Intercept = 1,
                     log.bathym = terra::values(cov_list$bathym) %>%
                       abs() %>%
                       log(),
                     log.bathym2 = terra::values(cov_list$bathym) %>%
                       abs() %>%
                       log() %>%
                       apply(., 2, function(x) x^2),
                     log.npp = terra::values(cov_list$npp$`2020-09-01`) %>%
                       log(),
                     log.npp2 = terra::values(cov_list$npp$`2020-09-01`) %>%
                       log() %>%
                       apply(., 2, function(x) x^2),
                     log.sst = terra::values(cov_list$sst$`2020-09-01`) %>%
                       log(),
                     log.sst2 = terra::values(cov_list$sst$`2020-09-01`) %>%
                       log() %>%
                       apply(., 2, function(x) x^2))
names(newdat) <- c('log.bathym','log.bathym2','log.npp','log.npp2','log.sst','log.sst2')

coeff1 <- hglm.fit$summary.fixed$mean
mean.pred <- as.matrix(newdat) %*% coeff1  #make predictions


hglm.pred <- cov_list$bathym
names(hglm.pred) <- "pred"
terra::values(hglm.pred) <- mean.pred



## HGAM

newdat <- data.frame(log.bathym = terra::values(cov_list$bathym) %>%
                        abs() %>%
                        log(),
                      log.npp = terra::values(cov_list$npp$`2020-09-01`) %>%
                        log(),
                      log.sst = terra::values(cov_list$sst$`2020-09-01`) %>%
                        log(),
                     id = unique(rsf.pts_10$id)[45])
names(newdat)[1:3] <- c('log.bathym','log.npp','log.sst')


mean.pred <- predict.bam(hgam.fit, newdata = newdat, type = "terms", terms = c("s(log.bathym)","s(log.npp)","s(log.sst)"),
                         discrete = FALSE, na.action = na.pass, se.fit = TRUE)

hgam.pred <- cov_list$bathym
names(hgam.pred) <- "pred"
terra::values(hgam.pred) <- rowSums(mean.pred$fit) + hgam.fit$coefficients[1]



## BRT

newdat <- data.frame(bathym = terra::values(cov_list$bathym) %>%
                       abs(),
                     npp = terra::values(cov_list$npp$`2020-09-01`),
                     sst = terra::values(cov_list$sst$`2020-09-01`))
names(newdat)[1:3] <- c('bathym','npp','sst')


mean.pred <- predict.gbm(brt.fit, newdata = newdat, n.trees = brt.fit$n.trees)

brt.pred <- cov_list$bathym
names(brt.pred) <- "pred"
terra::values(brt.pred) <- mean.pred


## HGPR

rast.sep.20 <- data.frame(log.bathym = log(abs(terra::values(cov_list$bathym))) %>%
                            as.vector(),
                          log.npp = log(terra::values(cov_list$npp$`2020-09-01`)) %>%
                            as.vector(),
                          log.sst = log(terra::values(cov_list$sst$`2020-09-01`)) %>%
                            as.vector()) %>%
  mutate(row_id = 1:nrow(.)) %>%
  drop_na(log.bathym, log.npp, log.sst)


# Define vector of covar names
covars <- c("log.bathym","log.npp","log.sst")

# Define 1D meshes to be used for prediction across sites
mesh.seq <- list(log.bathym = c(0.001, 5500),
                 log.npp = c(20, 200000),
                 log.sst = c(12,38)) %>%
  map(log)


# Define 1D mesh per covar
mesh.list <- vector("list", length(covars))
for (i in 1:length(covars)) {
  mesh.list[[i]] <- inla.mesh.1d(seq(mesh.seq[[i]][1], mesh.seq[[i]][2],
                                     length.out = 5),
                                 degree = 2,
                                 boundary = 'free')
}


# Generate matrices for covariate raster data (for prediction)
A.pop.sep.20 <- vector("list", length(covars))
for (i in 1:length(covars)) { #one matrix for model estimation and another for generating predictions for plotting
  A.pop.sep.20[[i]] <- inla.spde.make.A(mesh.list[[i]], loc = rast.sep.20[[covars[[i]]]])
}

# Store resulting GP coeffs per covar into a list
pred.coeffs <- hgpr.fit$summary.random[1:3] %>%
  map(., ~pull(.x, mean))

# Make predictions via linear algebra
pred.sep.20 <- A.pop.sep.20 %>%
  map2(.x = ., .y = pred.coeffs,
       ~{.x %*% .y %>%
           as.vector()}
      ) %>%
  bind_cols() %>%
  rowSums()  #sum up all predictions across covars


# Define dummy raster for storing predictions
hgpr.pred <- cov_list$bathym
names(hgpr.pred) <- "pred"
terra::values(hgpr.pred) <- NA  # initially store all NAs for locs w/o predictions
terra::values(hgpr.pred)[rast.sep.20$row_id] <- pred.sep.20

```

```{r}
#| label: map-gom-preds
#| column: page
#| fig-cap: "Predictions from each of the four models are mapped onto space for 2020-09 at the population-level. Note that each of the legends have different scales, but that the color scale (i.e., relative intensity) should be treated as comparable across results. Blue points in each plot indicate the points of 'resident' locations from three individuals that were tracked during this month-year."


bbox <- ext(hglm.pred)

tmp.pts <- rsf.pts_10 %>%
  filter(month.year == "2020-09-01", obs == 1)



p.hglm.gom <- ggplot() +
  geom_spatraster(data = hglm.pred, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = gom.sf) +
  geom_point(data = tmp.pts, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGLM") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))


p.hgam.gom <- ggplot() +
  geom_spatraster(data = hgam.pred, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = gom.sf) +
  geom_point(data = tmp.pts, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGAM") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))


p.brt.gom <- ggplot() +
  geom_spatraster(data = brt.pred, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = gom.sf) +
  geom_point(data = tmp.pts, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "BRT") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))


p.hgpr.gom <- ggplot() +
  geom_spatraster(data = hgpr.pred, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = gom.sf) +
  geom_point(data = tmp.pts, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGPR") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))



# Make composite plot
p.hglm.gom + p.hgam.gom + p.brt.gom + p.hgpr.gom +
  plot_layout(ncol = 2) +
  plot_annotation(title = 'Model predictions in GoM for 2020-09')

```

## Methods

I will not be presenting the details of how each method was parameterized or fit in this document, but I will show example predictive surfaces across methods in addition to a comparison of the metrics of predictive accuracy (i.e., transferability). For this project, I evaluated model transferability using the Boyce Index calculated per month-year, which essentially divides the predicted values from each model into 10 bins of equal width and then calculates the Spearman correlation of the predicted/expected ratio of points found within each bin. This index ranges from -1 to +1, where values close to -1 indicate a model predictive of where the animal isn't, values close to 0 are no different from random, and values close to +1 indicate a perfectly predictive model. Additionally, I calculated another metric that determines the number of bins required to account for $\geq 90 \%$ of all observations. This second method is included to potentially distinguish between methods with high correlations from the Boyce Index, but few observations in areas of greatest predicted suitability.

## Results

### Example of predictive surfaces

As was shown prior for the predictive surfaces in the Gulf of Mexico (on the training data), here I will show how each of the models predict log(intensity) over space for a single month-year in Brazil and Qatar separately.

```{r}
#| label: br-qa-preds

##############
### Brazil ###
##############

### HGLM

# Define coeff values from HGLM
coeff1 <- hglm.fit$summary.fixed$mean

# Subset covars by month.year
  vars <- data.frame(intercept = 1,
                     log.bathym = as.vector(terra::values(cov_list_br$bathym)) %>%
                       abs() %>%
                       log(),
                     log.bathym2 = as.vector(terra::values(cov_list_br$bathym)) %>%
                       abs() %>%
                       log() %>%
                       sapply(., function(x) x^2),
                     log.npp = as.vector(terra::values(cov_list_br$npp$`2016-06-01`)) %>%
                       log(),
                     log.npp2 = as.vector(terra::values(cov_list_br$npp$`2016-06-01`)) %>%
                       log() %>%
                       sapply(.,function(x) x^2),
                     log.sst = as.vector(terra::values(cov_list_br$sst$`2016-06-01`)) %>%
                       log(),
                     log.sst2 = as.vector(terra::values(cov_list_br$sst$`2016-06-01`)) %>%
                       log() %>%
                       sapply(., function(x) x^2))
  
  # Make predictions on intensity of use from model
  br.hglm <- as.matrix(vars) %*% coeff1  #make predictions

  # Store results in raster stack
  br.rast.hglm <- cov_list_br$bathym
  names(br.rast.hglm) <- "pred"
  terra::values(br.rast.hglm) <- br.hglm
  
  
  

### HGAM
  
newdat <- data.frame(log.bathym = terra::values(cov_list_br$bathym) %>%
                        abs() %>%
                        log(),
                      log.npp = terra::values(cov_list_br$npp$`2016-06-01`) %>%
                        log(),
                      log.sst = terra::values(cov_list_br$sst$`2016-06-01`) %>%
                        log(),
                     id = unique(rsf.pts_10$id)[45])
names(newdat)[1:3] <- c('log.bathym','log.npp','log.sst')


br.hgam <- predict.bam(hgam.fit, newdata = newdat, type = "terms", terms = c("s(log.bathym)","s(log.npp)","s(log.sst)"),
                         discrete = FALSE, na.action = na.pass, se.fit = TRUE)

# Store results in raster stack
  br.rast.hgam <- cov_list_br$bathym
  names(br.rast.hgam) <- "pred"
  terra::values(br.rast.hgam) <- rowSums(br.hgam$fit) + hgam.fit$coefficients[1]
  
  

  
### BRT
  
newdat <- data.frame(bathym = terra::values(cov_list_br$bathym) %>%
                       abs(),
                     npp = terra::values(cov_list_br$npp$`2016-06-01`),
                     sst = terra::values(cov_list_br$sst$`2016-06-01`))
names(newdat)[1:3] <- c('bathym','npp','sst')


br.brt <- predict.gbm(brt.fit, newdata = newdat, n.trees = brt.fit$n.trees)

br.rast.brt <- cov_list_br$bathym
names(br.rast.brt) <- "pred"
terra::values(br.rast.brt) <- br.brt




### HGPR

vars <- data.frame(log.bathym = log(abs(terra::values(cov_list_br$bathym))) %>%
                            as.vector(),
                          log.npp = log(terra::values(cov_list_br$npp$`2016-06-01`)) %>%
                            as.vector(),
                          log.sst = log(terra::values(cov_list_br$sst$`2016-06-01`)) %>%
                            as.vector()) %>%
  mutate(row_id = 1:nrow(.)) %>%
  drop_na(log.bathym, log.npp, log.sst)

vars2 <- data.frame(Intercept = 1,
                      log.sst = as.vector(terra::values(cov_list_br$sst$`2016-06-01`)) %>%
                        log(),
                      log.sst2 = as.vector(terra::values(cov_list_br$sst$`2016-06-01`)) %>%
                        log() %>%
                        . ^ 2) %>%
    mutate(row_id = 1:nrow(.)) %>%
    filter(row_id %in% vars$row_id)


# Generate matrices for covariate raster data (for prediction)
covars <- c("log.bathym","log.npp","log.sst")
A.pop.br <- vector("list", length(covars))
for (i in 1:length(covars)) { #one matrix for model estimation and another for generating predictions for plotting
  A.pop.br[[i]] <- inla.spde.make.A(mesh.list[[i]], loc = vars[[covars[[i]]]])
}

# Store resulting GP coeffs per covar into a list
pred.coeffs <- hgpr.fit$summary.random[1:3] %>%
  map(., ~pull(.x, mean))

# Define coeff values of fixed terms from HGPR
coeff2 <- hgpr.fit$summary.fixed$mean

# Make predictions via linear algebra
br.hgpr <- A.pop.br %>%
  map2(.x = ., .y = pred.coeffs,
       ~{.x %*% .y %>%
           as.vector()}
      ) %>%
  bind_cols() %>%
  rowSums()  #sum up all predictions across covars

# Make predictions using linear terms
  br.hgpr2 <- as.matrix(vars2[,1:3]) %*% coeff2


# Define dummy raster for storing predictions
br.rast.hgpr <- cov_list_br$bathym
names(br.rast.hgpr) <- "pred"
terra::values(br.rast.hgpr) <- NA  # initially store all NAs for locs w/o predictions
terra::values(br.rast.hgpr)[vars$row_id] <- br.hgpr + br.hgpr2[,1]










##############
### Qatar ###
##############

### HGLM

# Define coeff values from HGLM
coeff1 <- hglm.fit$summary.fixed$mean

# Subset covars by month.year
  vars <- data.frame(intercept = 1,
                     log.bathym = as.vector(terra::values(cov_list_qa$bathym)) %>%
                       abs() %>%
                       log(),
                     log.bathym2 = as.vector(terra::values(cov_list_qa$bathym)) %>%
                       abs() %>%
                       log() %>%
                       sapply(., function(x) x^2),
                     log.npp = as.vector(terra::values(cov_list_qa$npp$`2014-03-01`)) %>%
                       log(),
                     log.npp2 = as.vector(terra::values(cov_list_qa$npp$`2014-03-01`)) %>%
                       log() %>%
                       sapply(.,function(x) x^2),
                     log.sst = as.vector(terra::values(cov_list_qa$sst$`2014-03-01`)) %>%
                       log(),
                     log.sst2 = as.vector(terra::values(cov_list_qa$sst$`2014-03-01`)) %>%
                       log() %>%
                       sapply(., function(x) x^2))
  
  # Make predictions on intensity of use from model
  qa.hglm <- as.matrix(vars) %*% coeff1  #make predictions

  # Store results in raster stack
  qa.rast.hglm <- cov_list_qa$bathym
  names(qa.rast.hglm) <- "pred"
  terra::values(qa.rast.hglm) <- qa.hglm
  
  
  

### HGAM
  
newdat <- data.frame(log.bathym = terra::values(cov_list_qa$bathym) %>%
                        abs() %>%
                        log(),
                      log.npp = terra::values(cov_list_qa$npp$`2014-03-01`) %>%
                        log(),
                      log.sst = terra::values(cov_list_qa$sst$`2014-03-01`) %>%
                        log(),
                     id = unique(rsf.pts_10$id)[45])
names(newdat)[1:3] <- c('log.bathym','log.npp','log.sst')


qa.hgam <- predict.bam(hgam.fit, newdata = newdat, type = "terms", terms = c("s(log.bathym)","s(log.npp)","s(log.sst)"),
                         discrete = FALSE, na.action = na.pass, se.fit = TRUE)

# Store results in raster stack
  qa.rast.hgam <- cov_list_qa$bathym
  names(qa.rast.hgam) <- "pred"
  terra::values(qa.rast.hgam) <- rowSums(qa.hgam$fit) + hgam.fit$coefficients[1]
  
  

  
### BRT
  
newdat <- data.frame(bathym = terra::values(cov_list_qa$bathym) %>%
                       abs(),
                     npp = terra::values(cov_list_qa$npp$`2014-03-01`),
                     sst = terra::values(cov_list_qa$sst$`2014-03-01`))
names(newdat)[1:3] <- c('bathym','npp','sst')


qa.brt <- predict.gbm(brt.fit, newdata = newdat, n.trees = brt.fit$n.trees)

qa.rast.brt <- cov_list_qa$bathym
names(qa.rast.brt) <- "pred"
terra::values(qa.rast.brt) <- qa.brt




### HGPR

vars <- data.frame(log.bathym = log(abs(terra::values(cov_list_qa$bathym))) %>%
                            as.vector(),
                          log.npp = log(terra::values(cov_list_qa$npp$`2014-03-01`)) %>%
                            as.vector(),
                          log.sst = log(terra::values(cov_list_qa$sst$`2014-03-01`)) %>%
                            as.vector()) %>%
  mutate(row_id = 1:nrow(.)) %>%
  drop_na(log.bathym, log.npp, log.sst)

vars2 <- data.frame(Intercept = 1,
                      log.sst = as.vector(terra::values(cov_list_qa$sst$`2014-03-01`)) %>%
                        log(),
                      log.sst2 = as.vector(terra::values(cov_list_qa$sst$`2014-03-01`)) %>%
                        log() %>%
                        . ^ 2) %>%
    mutate(row_id = 1:nrow(.)) %>%
    filter(row_id %in% vars$row_id)


# Generate matrices for covariate raster data (for prediction)
covars <- c("log.bathym","log.npp","log.sst")
A.pop.qa <- vector("list", length(covars))
for (i in 1:length(covars)) { #one matrix for model estimation and another for generating predictions for plotting
  A.pop.qa[[i]] <- inla.spde.make.A(mesh.list[[i]], loc = vars[[covars[[i]]]])
}

# Store resulting GP coeffs per covar into a list
pred.coeffs <- hgpr.fit$summary.random[1:3] %>%
  map(., ~pull(.x, mean))

# Define coeff values of fixed terms from HGPR
coeff2 <- hgpr.fit$summary.fixed$mean

# Make predictions via linear algeqaa
qa.hgpr <- A.pop.qa %>%
  map2(.x = ., .y = pred.coeffs,
       ~{.x %*% .y %>%
           as.vector()}
      ) %>%
  bind_cols() %>%
  rowSums()  #sum up all predictions across covars

# Make predictions using linear terms
  qa.hgpr2 <- as.matrix(vars2[,1:3]) %*% coeff2


# Define dummy raster for storing predictions
qa.rast.hgpr <- cov_list_qa$bathym
names(qa.rast.hgpr) <- "pred"
terra::values(qa.rast.hgpr) <- NA  # initially store all NAs for locs w/o predictions
terra::values(qa.rast.hgpr)[vars$row_id] <- qa.hgpr + qa.hgpr2[,1]

```

```{r}
#| label: map-br-qa-preds
#| column: page
#| fig-cap: "Predictions from each of the four models are mapped onto space at the population-level in Brazil and Qatar. Note that each of the legends have different scales, but that the color scale (i.e., relative intensity) should be treated as comparable across results. Blue points in each plot indicate the points of 'resident' locations from individuals that were tracked during the selected month-year."


### Brazil ###

bbox <- ext(br.rast.hglm)

tmp.pts.br <- dat.br %>%
  filter(month.year == "2016-06-01")



p.hglm.br <- ggplot() +
  geom_spatraster(data = br.rast.hglm, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = br.sf) +
  geom_point(data = tmp.pts.br, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGLM") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))

p.hgam.br <- ggplot() +
  geom_spatraster(data = br.rast.hgam, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = br.sf) +
  geom_point(data = tmp.pts.br, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGAM") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))

p.brt.br <- ggplot() +
  geom_spatraster(data = br.rast.brt, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = br.sf) +
  geom_point(data = tmp.pts.br, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "BRT") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))

p.hgpr.br <- ggplot() +
  geom_spatraster(data = br.rast.hgpr, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = br.sf) +
  geom_point(data = tmp.pts.br, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGPR") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))



### Qatar ###

bbox <- ext(qa.rast.hglm)

tmp.pts.qa <- dat.qa %>%
  filter(month.year == "2014-03-01")


p.hglm.qa <- ggplot() +
  geom_spatraster(data = qa.rast.hglm, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = qa.sf) +
  geom_point(data = tmp.pts.qa, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGLM") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))

p.hgam.qa <- ggplot() +
  geom_spatraster(data = qa.rast.hgam, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = qa.sf) +
  geom_point(data = tmp.pts.qa, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGAM") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))

p.brt.qa <- ggplot() +
  geom_spatraster(data = qa.rast.brt, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = qa.sf) +
  geom_point(data = tmp.pts.qa, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "BRT") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))

p.hgpr.qa <- ggplot() +
  geom_spatraster(data = qa.rast.hgpr, aes(fill = pred)) +
  scale_fill_viridis_c("log(Intensity)", option = 'inferno') +
  geom_sf(data = qa.sf) +
  geom_point(data = tmp.pts.qa, aes(x, y), color = "blue", alpha = 0.7, size = 1) +
  labs(x="",y="", title = "HGPR") +
  theme_bw() +
  coord_sf(xlim = c(bbox[1], bbox[2]),
           ylim = c(bbox[3], bbox[4]),
           expand = FALSE) +
  theme(plot.title = element_text(size = 16, face = "bold"))




# Make composite plots
p.hglm.br + p.hgam.br + p.brt.br + p.hgpr.br +
  plot_layout(ncol = 2) +
  plot_annotation(title = 'Model predictions in Brazil for 2016-06')

p.hglm.qa + p.hgam.qa + p.brt.qa + p.hgpr.qa +
  plot_layout(ncol = 2) +
  plot_annotation(title = 'Model predictions in Qatar for 2014-03')
```



### Evaluating model transferability

The above predictions shown for the Brazil and Qatar data were performed for every month-year of the respective datasets, where time-matched observations were then used to extract these predicted estimates of log(intensity) per method. For each location, the range of all possible log(intensity) values were broken up into 10 bins of equal width per method (i.e, HGLM, HGAM, HGPR, BRT) and a ratio of predicted:expected observations was calculated per bin. A Spearman correlation was then calculated for the predicted:expected ratio over the binned values of log(intensity) to calculate the Boyce Index, which ranged from -1 to +1. 

Inspection of some of these plots for the Boyce Index showed that correlations could be relatively high despite the observed points falling in the middle of the predicted log(intensity) bin range, rather than being the highest at the greatest predicted values. Therefore, this method was supplemented by calculating the average number of bins that accounted for $\geq 90\%$ of the observations per site (Brazil, Qatar) per method, starting from the upper end of the distribution since this is where the greatest number of observations should be found for a highly predictive model.

Upon exploratory inspection of some of the predicted intensity surfaces from the models, it appeared that the spatial resolution of 4.5 km could not always properly represent the environmental conditions at the small island of Fernando de Noronha for the Brazil dataset. Since 20 individuals spent some or all of their time at this island, each of the model transferability assessments were calculated using all Brazil data ('Brazil_full') or only individuals tracked along the mainland ('Brazil_sub') to account for potentially biased estimates of transferability.


```{r}
#| label: calc-boyce-index

##############
### Brazil ###
##############

### HGLM

my.ind.br <- names(cov_list_br$npp)
br.rast.hglm <- rep(cov_list_br$bathym, nlyr(cov_list_br$npp))
names(br.rast.hglm) <- my.ind.br

# Define coeff values from HGLM
coeff1 <- hglm.fit$summary.fixed$mean

# Make spatial predictions per month.year
# tic()
for (i in 1:nlyr(cov_list_br$npp)) {

  # Subset covars by month.year
  vars <- data.frame(intercept = 1,
                     log.bathym = as.vector(terra::values(cov_list_br$bathym)) %>%
                       abs() %>%
                       log(),
                     log.bathym2 = as.vector(terra::values(cov_list_br$bathym)) %>%
                       abs() %>%
                       log() %>%
                       sapply(., function(x) x^2),
                     log.npp = as.vector(terra::values(cov_list_br$npp[[my.ind.br[i]]])) %>%
                       log(),
                     log.npp2 = as.vector(terra::values(cov_list_br$npp[[my.ind.br[i]]])) %>%
                       log() %>%
                       sapply(.,function(x) x^2),
                     log.sst = as.vector(terra::values(cov_list_br$sst[[my.ind.br[i]]])) %>%
                       log(),
                     log.sst2 = as.vector(terra::values(cov_list_br$sst[[my.ind.br[i]]])) %>%
                       log() %>%
                       sapply(., function(x) x^2))


  # Make predictions on intensity of use from model
  br.hglm <- as.matrix(vars) %*% coeff1  #make predictions

  # Store results in raster stack
  terra::values(br.rast.hglm[[i]]) <- br.hglm  #keep on log-scale since response scale results in crazy large values

}
# skrrrahh('khaled2')
# toc()  #took 1 min



# Assess model performance via Continuous Boyce Index
boyce.br.full.hglm <- vector("list", nlyr(br.rast.hglm))
boyce.br.sub.hglm <- vector("list", nlyr(br.rast.hglm))
# tic()
for (i in 1:nlyr(br.rast.hglm)) {

  # Subset tracks by month.year
  obs_full <- dat.br %>%
    filter(month.year == my.ind.br[i]) %>%
    dplyr::select(x, y)

  obs_sub <- dat.br %>%
    filter(month.year == my.ind.br[i], x < -3650000) %>%
    dplyr::select(x, y)

  boyce.br.full.hglm[[i]] <- boyce(fit = br.rast.hglm[[i]],
                                   obs = obs_full,
                                   nbins = 10,
                                   bin.method = "seq",
                                   PEplot = FALSE,
                                   rm.duplicate = FALSE,
                                   method = "spearman")

  boyce.br.sub.hglm[[i]] <- boyce(fit = br.rast.hglm[[i]],
                                  obs = obs_sub,
                                  nbins = 10,
                                  bin.method = "seq",
                                  PEplot = FALSE,
                                  rm.duplicate = FALSE,
                                  method = "spearman")
}
# skrrrahh("khaled3")
# toc()  #took 4.8 sec

perc.use.br.full.hglm <- boyce.br.full.hglm %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))

perc.use.br.sub.hglm <- boyce.br.sub.hglm %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))


boyce.br.full.hglm <- boyce.br.full.hglm %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Brazil_all",
             Method = "HGLM")

boyce.br.sub.hglm <- boyce.br.sub.hglm %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Brazil_sub",
             Method = "HGLM")




### HGAM

my.ind.br <- names(cov_list_br$npp)
br.rast.hgam <- rep(cov_list_br$bathym, nlyr(cov_list_br$npp))
names(br.rast.hgam) <- my.ind.br

# tic()
for (i in 1:nlyr(cov_list_br$npp)) {

  # Subset covars by month.year
  vars <- data.frame(log.bathym = as.vector(terra::values(cov_list_br$bathym)) %>%
                       abs() %>%
                       log(),
                     log.npp = as.vector(terra::values(cov_list_br$npp[[my.ind.br[i]]])) %>%
                       log(),
                     log.sst = as.vector(terra::values(cov_list_br$sst[[my.ind.br[i]]])) %>%
                       log(),
                     id = hgam.fit$var.summary$id)


  # Make predictions on intensity of use from model
  br.hgam <- predict.bam(hgam.fit, newdata = vars, type = "terms", terms = c("s(log.bathym)","s(log.npp)","s(log.sst)"),
                         discrete = FALSE, na.action = na.pass)

  terra::values(br.rast.hgam[[i]]) <- rowSums(br.hgam + hgam.fit$coefficients[1])  #add intercept

}
# skrrrahh('khaled2')
# toc()  #took 1.25 min



# Assess model performance via Continuous Boyce Index
boyce.br.full.hgam <- vector("list", nlyr(br.rast.hgam))
boyce.br.sub.hgam <- vector("list", nlyr(br.rast.hgam))
# tic()
for (i in 1:nlyr(br.rast.hgam)) {

  # Subset tracks by month.year
  obs_full <- dat.br %>%
    filter(month.year == my.ind.br[i]) %>%
    dplyr::select(x, y)

  obs_sub <- dat.br %>%
    filter(month.year == my.ind.br[i], x < -3650000) %>%
    dplyr::select(x, y)

  boyce.br.full.hgam[[i]] <- boyce(fit = br.rast.hgam[[i]],
                                   obs = obs_full,
                                   nbins = 10,
                                   bin.method = "seq",
                                   PEplot = FALSE,
                                   rm.duplicate = FALSE,
                                   method = "spearman")

  boyce.br.sub.hgam[[i]] <- boyce(fit = br.rast.hgam[[i]],
                                  obs = obs_sub,
                                  nbins = 10,
                                  bin.method = "seq",
                                  PEplot = FALSE,
                                  rm.duplicate = FALSE,
                                  method = "spearman")
}
# skrrrahh("khaled3")
# toc()  #took 4.5 sec



perc.use.br.full.hgam <- boyce.br.full.hgam %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))

perc.use.br.sub.hgam <- boyce.br.sub.hgam %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))


boyce.br.full.hgam <- boyce.br.full.hgam %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Brazil_all",
             Method = "HGAM")

boyce.br.sub.hgam <- boyce.br.sub.hgam %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Brazil_sub",
             Method = "HGAM")




### BRT

my.ind.br <- names(cov_list_br$npp)
br.rast.brt <- rep(cov_list_br$bathym, nlyr(cov_list_br$npp))
terra::values(br.rast.brt) <- NA  # initially store all NAs for locs w/o predictions
names(br.rast.brt) <- my.ind.br

# tic()
for (i in 1:nlyr(cov_list_br$npp)) {

  # print(paste0(i,"/",nlyr(cov_list_br$npp)))

  # Subset covars by month.year
  vars <- data.frame(bathym = as.vector(terra::values(cov_list_br$bathym)) %>%
                       abs(),
                     npp = as.vector(terra::values(cov_list_br$npp[[my.ind.br[i]]])),
                     sst = as.vector(terra::values(cov_list_br$sst[[my.ind.br[i]]]))) %>%
    mutate(row_id = 1:nrow(.)) %>%
    drop_na(bathym, npp, sst)


  # Make predictions on intensity of use from model
  br.brt <- predict.gbm(brt.fit, newdata = vars[,-4], n.trees = brt.fit$n.trees)
  terra::values(br.rast.brt[[i]])[vars$row_id] <- br.brt  #keep on log-scale since response scale results in crazy large values

}
# skrrrahh('khaled2')
# toc()  #took 5.5 min



# Assess model performance via Continuous Boyce Index
boyce.br.full.brt <- vector("list", nlyr(br.rast.brt))
boyce.br.sub.brt <- vector("list", nlyr(br.rast.brt))
# tic()
for (i in 1:nlyr(br.rast.brt)) {

  # Subset tracks by month.year
  obs_full <- dat.br %>%
    filter(month.year == my.ind.br[i]) %>%
    dplyr::select(x, y)

  obs_sub <- dat.br %>%
    filter(month.year == my.ind.br[i], x < -3650000) %>%
    dplyr::select(x, y)

  boyce.br.full.brt[[i]] <- boyce(fit = br.rast.brt[[i]],
                                  obs = obs_full,
                                  nbins = 10,
                                  bin.method = "seq",
                                  PEplot = FALSE,
                                  rm.duplicate = FALSE,
                                  method = "spearman")

  boyce.br.sub.brt[[i]] <- boyce(fit = br.rast.brt[[i]],
                                 obs = obs_sub,
                                 nbins = 10,
                                 bin.method = "seq",
                                 PEplot = FALSE,
                                 rm.duplicate = FALSE,
                                 method = "spearman")
}
# skrrrahh("khaled3")
# toc()  #took 5 sec


perc.use.br.full.brt <- boyce.br.full.brt %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))

perc.use.br.sub.brt <- boyce.br.sub.brt %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))


boyce.br.full.brt <- boyce.br.full.brt %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Brazil_all",
             Method = "BRT")

boyce.br.sub.brt <- boyce.br.sub.brt %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Brazil_sub",
             Method = "BRT")




### HGPR

# Define vector of covar names
covars <- c("log.bathym","log.npp","log.sst")

# Define 1D meshes to be used for prediction across sites
mesh.seq <- list(log.bathym = c(0.001, 5500),
                 log.npp = c(20, 200000),
                 log.sst = c(12,38)) %>%
  map(log)

alpha <- 2  #for calculating Matern covariance matrix


# Define 1D mesh per covar
mesh.list <- vector("list", length(covars))
for (i in 1:length(covars)) {
  mesh.list[[i]] <- inla.mesh.1d(seq(mesh.seq[[i]][1], mesh.seq[[i]][2],
                                     length.out = 5),
                                 degree = 2,
                                 boundary = 'free')
}




my.ind.br <- names(cov_list_br$npp)
br.rast.hgpr <- rep(cov_list_br$bathym, nlyr(cov_list_br$npp))
names(br.rast.hgpr) <- my.ind.br

# Define coeff values from HGPR
coeff1 <- hgpr.fit$summary.random[1:3] %>%
  map(., ~pull(.x, mean))

# Define coeff values of fixed terms from HGPR
coeff2 <- hgpr.fit$summary.fixed$mean


# Make spatial predictions per month.year
# tic()
for (i in 1:nlyr(cov_list_br$npp)) {

  # Subset covars by month.year
  vars <- data.frame(log.bathym = as.vector(terra::values(cov_list_br$bathym)) %>%
                       abs() %>%
                       log(),
                     log.npp = as.vector(terra::values(cov_list_br$npp[[my.ind.br[i]]])) %>%
                       log(),
                     log.sst = as.vector(terra::values(cov_list_br$sst[[my.ind.br[i]]])) %>%
                       log()) %>%
    mutate(row_id = 1:nrow(.)) %>%
    drop_na(log.bathym, log.npp, log.sst)

  vars2 <- data.frame(Intercept = 1,
                      log.sst = as.vector(terra::values(cov_list_br$sst[[my.ind.br[i]]])) %>%
                        log(),
                      log.sst2 = as.vector(terra::values(cov_list_br$sst[[my.ind.br[i]]])) %>%
                        log() %>%
                        . ^ 2) %>%
    mutate(row_id = 1:nrow(.)) %>%
    filter(row_id %in% vars$row_id)



  # Generate matrices for covariate raster data (for prediction)
  A.mat <- vector("list", length(covars))
  for (i in 1:length(covars)) { #one matrix for model estimation and another for generating predictions for plotting
    A.mat[[i]] <- inla.spde.make.A(mesh.list[[i]], loc = vars[[covars[[i]]]])
  }


  # Make predictions on intensity of use from model for GP terms
  br.hgpr <- A.mat %>%
    map2(.x = ., .y = coeff1,
         ~{.x %*% .y %>%
             as.vector()}
    ) %>%
    bind_cols() %>%
    rowSums()  #sum up all predictions across covars

  # Make predictions using linear terms
  br.hgpr2 <- as.matrix(vars2[,1:3]) %*% coeff2

  # Store results in raster stack
  terra::values(br.rast.hgpr[[i]]) <- NA  # initially store all NAs for locs w/o predictions
  terra::values(br.rast.hgpr[[i]])[vars$row_id] <- br.hgpr + br.hgpr2[,1]
}
# skrrrahh('khaled2')
# toc()  #took 40 sec



# Assess model performance via Continuous Boyce Index
boyce.br.full.hgpr <- vector("list", nlyr(br.rast.hgpr))
boyce.br.sub.hgpr <- vector("list", nlyr(br.rast.hgpr))
# tic()
for (i in 1:nlyr(br.rast.hgpr)) {

  # Subset tracks by month.year
  obs_full <- dat.br %>%
    filter(month.year == my.ind.br[i]) %>%
    dplyr::select(x, y)

  obs_sub <- dat.br %>%
    filter(month.year == my.ind.br[i], x < -3650000) %>%
    dplyr::select(x, y)

  boyce.br.full.hgpr[[i]] <- boyce(fit = br.rast.hgpr[[i]],
                                   obs = obs_full,
                                   nbins = 10,
                                   bin.method = "seq",
                                   PEplot = FALSE,
                                   rm.duplicate = FALSE,
                                   method = "spearman")

  boyce.br.sub.hgpr[[i]] <- boyce(fit = br.rast.hgpr[[i]],
                                  obs = obs_sub,
                                  nbins = 10,
                                  bin.method = "seq",
                                  PEplot = FALSE,
                                  rm.duplicate = FALSE,
                                  method = "spearman")
}
# skrrrahh("khaled3")
# toc()  #took 5 sec



perc.use.br.full.hgpr <- boyce.br.full.hgpr %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))

perc.use.br.sub.hgpr <- boyce.br.sub.hgpr %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))


boyce.br.full.hgpr <- boyce.br.full.hgpr %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Brazil_all",
             Method = "HGPR")

boyce.br.sub.hgpr <- boyce.br.sub.hgpr %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Brazil_sub",
             Method = "HGPR")





#############
### Qatar ###
#############

### HGLM

my.ind.qa <- names(cov_list_qa$npp)
qa.rast.hglm <- rep(cov_list_qa$bathym, nlyr(cov_list_qa$npp))
names(qa.rast.hglm) <- my.ind.qa

# Define coeff values from HGLM
coeff1 <- hglm.fit$summary.fixed$mean

# Make spatial predictions per month.year
# tic()
for (i in 1:nlyr(cov_list_qa$npp)) {

  # Subset covars by month.year
  vars <- data.frame(intercept = 1,
                     log.bathym = as.vector(terra::values(cov_list_qa$bathym)) %>%
                       abs() %>%
                       log(),
                     log.bathym2 = as.vector(terra::values(cov_list_qa$bathym)) %>%
                       abs() %>%
                       log() %>%
                       sapply(., function(x) x^2),
                     log.npp = as.vector(terra::values(cov_list_qa$npp[[my.ind.qa[i]]])) %>%
                       log(),
                     log.npp2 = as.vector(terra::values(cov_list_qa$npp[[my.ind.qa[i]]])) %>%
                       log() %>%
                       sapply(.,function(x) x^2),
                     log.sst = as.vector(terra::values(cov_list_qa$sst[[my.ind.qa[i]]])) %>%
                       log(),
                     log.sst2 = as.vector(terra::values(cov_list_qa$sst[[my.ind.qa[i]]])) %>%
                       log() %>%
                       sapply(., function(x) x^2))


  # Make predictions on intensity of use from model
  qa.hglm <- as.matrix(vars) %*% coeff1  #make predictions

  # Store results in raster stack
  terra::values(qa.rast.hglm[[i]]) <- qa.hglm  #keep on log-scale since response scale results in crazy large values

}
# skrrrahh('khaled2')
# toc()  #took 1 sec



# Assess model performance via Continuous Boyce Index
boyce.qa.hglm <- vector("list", nlyr(qa.rast.hglm))
# tic()
for (i in 1:nlyr(qa.rast.hglm)) {

  # Subset tracks by month.year
  obs <- dat.qa %>%
    filter(month.year == my.ind.qa[i]) %>%
    dplyr::select(x, y)

  boyce.qa.hglm[[i]] <- boyce(fit = qa.rast.hglm[[i]],
                              obs = obs,
                              nbins = 10,
                              bin.method = "seq",
                              PEplot = FALSE,
                              rm.duplicate = FALSE,
                              method = "spearman")
  }
# skrrrahh("khaled3")
# toc()  #took 1 sec


perc.use.qa.hglm <- boyce.qa.hglm %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))


boyce.qa.hglm <- boyce.qa.hglm %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Qatar",
             Method = "HGLM")




### HGAM

my.ind.qa <- names(cov_list_qa$npp)
qa.rast.hgam <- rep(cov_list_qa$bathym, nlyr(cov_list_qa$npp))
names(qa.rast.hgam) <- my.ind.qa


# Make spatial predictions per month.year
# tic()
for (i in 1:nlyr(cov_list_qa$npp)) {

  # Subset covars by month.year
  vars <- data.frame(log.bathym = as.vector(terra::values(cov_list_qa$bathym)) %>%
                       abs() %>%
                       log(),
                     log.npp = as.vector(terra::values(cov_list_qa$npp[[my.ind.qa[i]]])) %>%
                       log(),
                     log.sst = as.vector(terra::values(cov_list_qa$sst[[my.ind.qa[i]]])) %>%
                       log(),
                     id = hgam.fit$var.summary$id)


  # Make predictions on intensity of use from model
  qa.hgam <- predict.bam(hgam.fit, newdata = vars, type = "terms", terms = c("s(log.bathym)","s(log.npp)","s(log.sst)"),
                         discrete = FALSE, na.action = na.pass)

  terra::values(qa.rast.hgam[[i]]) <- rowSums(qa.hgam + hgam.fit$coefficients[1])  #add intercept

}
# skrrrahh('khaled2')
# toc()  #took 1 sec



# Assess model performance via Continuous Boyce Index
boyce.qa.hgam <- vector("list", nlyr(qa.rast.hgam))
# tic()
for (i in 1:nlyr(qa.rast.hgam)) {

  # Subset tracks by month.year
  obs <- dat.qa %>%
    filter(month.year == my.ind.qa[i]) %>%
    dplyr::select(x, y)

  boyce.qa.hgam[[i]] <- boyce(fit = qa.rast.hgam[[i]],
                              obs = obs,
                              nbins = 10,
                              bin.method = "seq",
                              PEplot = FALSE,
                              rm.duplicate = FALSE,
                              method = "spearman")
}
# skrrrahh("khaled3")
# toc()  #took 1 sec

perc.use.qa.hgam <- boyce.qa.hgam %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))


boyce.qa.hgam <- boyce.qa.hgam %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Qatar",
             Method = "HGAM")




### BRT

my.ind.qa <- names(cov_list_qa$npp)
qa.rast.brt <- rep(cov_list_qa$bathym, nlyr(cov_list_qa$npp))
names(qa.rast.brt) <- my.ind.qa

# tic()
for (i in 1:nlyr(cov_list_qa$npp)) {

  # Subset covars by month.year
  vars <- data.frame(bathym = as.vector(terra::values(cov_list_qa$bathym)) %>%
                       abs(),
                     npp = as.vector(terra::values(cov_list_qa$npp[[my.ind.qa[i]]])),
                     sst = as.vector(terra::values(cov_list_qa$sst[[my.ind.qa[i]]]))) %>%
    mutate(row_id = 1:nrow(.)) %>%
    drop_na(bathym, npp, sst)


  # Make predictions on intensity of use from model
  qa.brt <- predict.gbm(brt.fit, newdata = vars[,-4], n.trees = brt.fit$n.trees)

  terra::values(qa.rast.brt[[i]]) <- NA  # initially store all NAs for locs w/o predictions
  terra::values(qa.rast.brt[[i]])[vars$row_id] <- qa.brt  #keep on log-scale since response scale results in crazy large values

}
# skrrrahh('khaled2')
# toc()  #took 2 sec




boyce.qa.brt <- vector("list", nlyr(qa.rast.brt))
# tic()
for (i in 1:nlyr(qa.rast.brt)) {

  # Subset tracks by month.year
  obs <- dat.qa %>%
    filter(month.year == my.ind.qa[i]) %>%
    dplyr::select(x, y)

  boyce.qa.brt[[i]] <- boyce(fit = qa.rast.brt[[i]],
                             obs = obs,
                             nbins = 10,
                             bin.method = "seq",
                             PEplot = FALSE,
                             rm.duplicate = FALSE,
                             method = "spearman")
}
# skrrrahh("khaled3")
# toc()  #took 1 sec


perc.use.qa.brt <- boyce.qa.brt %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))


boyce.qa.brt <- boyce.qa.brt %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Qatar",
             Method = "BRT")




### HGPR

my.ind.qa <- names(cov_list_qa$npp)
qa.rast.hgpr <- rep(cov_list_qa$bathym, nlyr(cov_list_qa$npp))
names(qa.rast.hgpr) <- my.ind.qa

# Define coeff values from HGPR
coeff1 <- hgpr.fit$summary.random[1:3] %>%
  map(., ~pull(.x, mean))

# Define coeff values of fixed terms from HGPR
coeff2 <- hgpr.fit$summary.fixed$mean



# Make spatial predictions per month.year
# tic()
for (i in 1:nlyr(cov_list_qa$npp)) {

  # Subset covars by month.year
  vars <- data.frame(log.bathym = as.vector(terra::values(cov_list_qa$bathym)) %>%
                       abs() %>%
                       log(),
                     log.npp = as.vector(terra::values(cov_list_qa$npp[[my.ind.qa[i]]])) %>%
                       log(),
                     log.sst = as.vector(terra::values(cov_list_qa$sst[[my.ind.qa[i]]])) %>%
                       log()) %>%
    mutate(row_id = 1:nrow(.)) %>%
    drop_na(log.bathym, log.npp, log.sst)

  vars2 <- data.frame(Intercept = 1,
                      log.sst = as.vector(terra::values(cov_list_qa$sst[[my.ind.qa[i]]])) %>%
                        log(),
                      log.sst2 = as.vector(terra::values(cov_list_qa$sst[[my.ind.qa[i]]])) %>%
                        log() %>%
                        . ^ 2) %>%
    mutate(row_id = 1:nrow(.)) %>%
    filter(row_id %in% vars$row_id)



  # Generate matrices for covariate raster data (for prediction)
  A.mat <- vector("list", length(covars))
  for (i in 1:length(covars)) { #one matrix for model estimation and another for generating predictions for plotting
    A.mat[[i]] <- inla.spde.make.A(mesh.list[[i]], loc = vars[[covars[[i]]]])
  }


  # Make predictions on intensity of use from model for GP terms
  qa.hgpr <- A.mat %>%
    map2(.x = ., .y = coeff1,
         ~{.x %*% .y %>%
             as.vector()}
    ) %>%
    bind_cols() %>%
    rowSums()  #sum up all predictions across covars

  # Make predictions using linear terms
  qa.hgpr2 <- as.matrix(vars2[,1:3]) %*% coeff2

  # Store results in raster stack
  terra::values(qa.rast.hgpr[[i]]) <- NA  # initially store all NAs for locs w/o predictions
  terra::values(qa.rast.hgpr[[i]])[vars$row_id] <- qa.hgpr + qa.hgpr2[,1]

}
# skrrrahh('khaled2')
# toc()  #took 2 sec



# Assess model performance via Continuous Boyce Index
boyce.qa.hgpr <- vector("list", nlyr(qa.rast.hgpr))
# tic()
for (i in 1:nlyr(qa.rast.hgpr)) {

  # Subset tracks by month.year
  obs <- dat.qa %>%
    filter(month.year == my.ind.qa[i]) %>%
    dplyr::select(x, y)

  boyce.qa.hgpr[[i]] <- boyce(fit = qa.rast.hgpr[[i]],
                              obs = obs,
                              nbins = 10,
                              bin.method = "seq",
                              PEplot = FALSE,
                              rm.duplicate = FALSE,
                              method = "spearman")
}
# skrrrahh("khaled3")
# toc()  #took 1 sec


perc.use.qa.hgpr <- boyce.qa.hgpr %>%
  map(., pluck, "perc.use") %>%
  set_names(1:length(.)) %>%
  bind_rows() %>%
  janitor::remove_empty(which = "cols") %>%
  apply(., 2, function(x) cumsum(rev(x)))


boyce.qa.hgpr <- boyce.qa.hgpr %>%
  map(., pluck, "cor") %>%
  unlist() %>%
  data.frame(cor = .,
             Region = "Qatar",
             Method = "HGPR")
```



```{r}
#| label: boyce-plots
#| column: page
#| fig-cap: "Violin plots of Boyce Index values per method and study region. The Brazil dataset has been split to evaluate model performance when including (Brazil_all) or excluding (Brazil_sub) observations at Fernando de Noronha. The black points indicate the mean values, whereas the colored points indicate the values per month-year of the dataset."

boyce.fit <- rbind(boyce.br.full.hglm, boyce.br.sub.hglm, boyce.qa.hglm,
                   boyce.br.full.hgam, boyce.br.sub.hgam, boyce.qa.hgam,
                   boyce.br.full.brt, boyce.br.sub.brt, boyce.qa.brt,
                   boyce.br.full.hgpr, boyce.br.sub.hgpr, boyce.qa.hgpr)

boyce.mean <- boyce.fit %>%
  group_by(Method, Region) %>%
  summarize(mean = mean(cor, na.rm = TRUE)) %>%
  ungroup()

ggplot(data = boyce.fit, aes(Region, cor)) +
  geom_point(aes(fill = Method), pch = 21, alpha = 0.7, size = 5, position = position_dodge(width = 0.75)) +
  geom_violin(aes(color = Method), fill = "transparent", position = position_dodge(width = 0.75)) +
  geom_point(data = boyce.mean, aes(x = Region, y = mean, group = Method),
             size = 6, position = position_dodge(width = 0.75)) +
  geom_hline(yintercept = 0, linewidth = 1) +
  lims(y = c(-1,1)) +
  labs(x="", y = "Boyce Index") +
  theme_bw()
```


```{r}
#| label: boyce-bin-plots
#| column: page
#| fig-cap: 
#|  - "Line plots showing the cumulative percentage of observations found in each bin, working backwards from the greatest predicted intensity (bin 10) to the lowest (bin 1). Color of the lines denote each of the month-years of predictions per site. The horizontal red dashed line indicates 90%."
#|  - "Plot showing the mean (+/- SD) number of bins that accounted for at least 90% of all observations. Fewer bins required to capture >90% of the data indicates a better performing model."

cum.perc <- list(Brazil_all.HGLM = perc.use.br.full.hglm,
                 Brazil_sub.HGLM = perc.use.br.sub.hglm,
                 Qatar.HGLM = perc.use.qa.hglm,
                 Brazil_all.HGAM = perc.use.br.full.hgam,
                 Brazil_sub.HGAM = perc.use.br.sub.hgam,
                 Qatar.HGAM = perc.use.qa.hgam,
                 Brazil_all.BRT = perc.use.br.full.brt,
                 Brazil_sub.BRT = perc.use.br.sub.brt,
                 Qatar.BRT = perc.use.qa.brt,
                 Brazil_all.HGPR = perc.use.br.full.hgpr,
                 Brazil_sub.HGPR = perc.use.br.sub.hgpr,
                 Qatar.HGPR = perc.use.qa.hgpr)


cum.perc.mean <- cum.perc %>%
  map(., ~apply(.x, 2, function(x) which(x >= 0.9)[1])) %>%
  map(., ~{data.frame(mean = mean(.x),
                sd = sd(.x)
                )}
      ) %>%
  bind_rows(.id = "id") %>%
  separate(col = id, sep = "\\.", into = c("Region","Method"))

cum.perc.long <- cum.perc %>% 
  map(., ~{.x %>% 
      data.frame() %>%
      mutate(bin = factor(10:1, levels = 10:1)) %>%
      pivot_longer(cols = -bin, names_to = 'month.year', values_to = "cum.perc")}
      ) %>% 
  bind_rows(.id = "id") %>% 
  separate(col = id, sep = "\\.", into = c("Region","Method")) %>% 
  mutate(across(Method, factor, levels = c('HGLM','HGAM','BRT','HGPR')))




ggplot(cum.perc.long, aes(bin, cum.perc)) +
  geom_hline(yintercept = 0.9, linewidth = 0.75, linetype = "dashed", color = "red") +
  geom_line(aes(group = month.year, color = month.year)) +
  labs(x = "Bin", y = "Cumulative Percentage") +
  theme_bw() +
  theme(legend.position = "none") +
  facet_grid(Region ~ Method, scales = "fixed")


ggplot(data = cum.perc.mean, aes(Region, mean)) +
  geom_linerange(aes(ymin = (mean - sd), ymax = (mean + sd), color = Method),
                 position = position_dodge(width = 0.75)) +
  geom_point(aes(group = Method, color = Method),
             size = 6, position = position_dodge(width = 0.75)) +
  # lims(y = c(0,10)) +
  labs(x="", y = "Avg # of bins accounting for 90% of obs") +
  theme_bw() +
  theme()
```



## Conclusions

Based on these findings from comparing the model transferability based on the statistical model that was used, it appears that the hierarchical Gaussian Process regression performed best based on both the Boyce Index and the mean number of bins to capture $\geq90\%$ of the data. This result held up for both study locations (Brazil and Qatar), although differences were small between the HGPR and HGLM models.

For the remaining two objectives of my project, I'll be using the HGPR model to evaluate the effect of spatial resolution and accounting for life stage preferences on model tranferability.
